#summary Frequently asked questions about the toolkit

On this page, we list several frequently asked questions about the toolkit (these are mostly excerpts from emails):
 * [#How_fast_is_the_toolkit,_e.g._for_transfer_entropy_estimation? How fast is the toolkit, e.g. for transfer entropy estimation?]
 * [#Can_the_Kraskov-Stoegbauer-Grassberger_estimator_add_noise_to_the_data? Can the Kraskov-Stoegbauer-Grassberger estimator add noise to the data?]

== How fast is the toolkit, e.g. for transfer entropy estimation? ==

I've had some enquiries regarding how long the toolkit will take to compute transfer entropy on large data sets, say 100 000 samples.
The answer depends mostly on the type of data that you are calculating transfer entropy on:
 # *Is it discrete or discretized data?* [http://code.google.com/p/information-dynamics-toolkit/source/browse/trunk/java/source/infodynamics/measures/discrete/ApparentTransferEntropyCalculator.java infodynamics.discrete.ApparentTransferEntropyCalculator] is very fast, on the order of 1 sec or less for this sized data.
 # *Is it continuous data (that you would prefer not to discretize)?* This is slower, using techniques such as a Kraskov-Grassberger estimator (this is the best of breed for continuous valued data), i.e. [http://code.google.com/p/information-dynamics-toolkit/source/browse/trunk/java/source/infodynamics/measures/continuous/kraskov/TransferEntropyCalculatorKraskov.java infodynamics.measures.continuous.kraskov.TransferEntropyCalculatorKraskov]. At the moment, a 100 000 time step calculation will take almost an hour.  This will be sped up significantly in the future once I implement a fast-neighbour search algorithm. In the meantime, if you want the calculation to run faster than that, then you could discretize your data (at the expense of accuracy), or subsample (say into 10 data sets of 10 000 time steps, which will each take ~30 secs, and average over these. This will be a little less accurate, but I'm guessing there won't be much in it, and you will get a standard error measurement from this approach as well). You can test out how long the code takes to run e.g. with [SimpleJavaExamples#Example_4_-_Transfer_entropy_on_continuous_data_using_Kraskov_es example4] in the simple java demos (mirrored in the simple octave/matlab demos [OctaveMatlabExamples#Example_4_-_Transfer_entropy_on_continuous_data_using_Kraskov_es here] and the python demos [PythonExamples#Example_4_-_Transfer_entropy_on_continuous_data_using_Kraskov_es here]), by altering the length of the random data that is used.

== Can the Kraskov-Stoegbauer-Grassberger estimator add noise to the data? ==

The [http://dx.doi.org/10.1103/PhysRevE.69.066138 original publication] of the KSG estimator recommends the addition of a very small amount of noise to the data (e.g 1e-8) to address the situation where multiple data points share the same value in (at least) one dimension.

This can be done for each KSG estimator in JIDT (for MI basec calculators from v1.0, for conditional MI based calculators from v1.1) as shown below for the transfer entropy estimator:
{{{
// The following is Java code; change it to your own language if required
TransferEntropyCalculatorKraskov teCalc =
		new TransferEntropyCalculatorKraskov();
teCalc.setProperty("NORMALISE", "true"); // Normalise the individual variables (default)
// Set up to add noise with standard deviation of 0.00000001 normalised units
teCalc.setProperty("NOISE_LEVEL_TO_ADD", "0.00000001");
// Then use the calculator as normal ...
}}}
More details are shown in the [Documentation Javadocs] for the `setProperty(String, String)` method for each relevant calculator.